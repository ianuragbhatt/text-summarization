{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianuragbhatt/text-summarization/blob/main/text-summarization-abstractive-T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gExvOjEL9o-M",
        "outputId": "1068dc18-42fb-4714-d843-78ee51787b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.20.0\n",
            "  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.20.0) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.20.0) (3.10.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.20.0\n",
        "!pip install keras_nlp==0.3.0\n",
        "!pip install datasets\n",
        "!pip install huggingface-hub\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11oLOHFj9sIx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Only log error messages\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBGGK7lo91_l"
      },
      "outputs": [],
      "source": [
        "# The percentage of the dataset you want to split as train and test\n",
        "TRAIN_TEST_SPLIT = 0.1\n",
        "\n",
        "MAX_INPUT_LENGTH = 1024  # Maximum length of the input to the model\n",
        "MIN_TARGET_LENGTH = 5  # Minimum length of the output by the model\n",
        "MAX_TARGET_LENGTH = 128  # Maximum length of the output by the model\n",
        "BATCH_SIZE = 8  # Batch-size for training our model\n",
        "LEARNING_RATE = 2e-5  # Learning-rate for training our model\n",
        "MAX_EPOCHS = 1  # Maximum number of epochs we will train the model for\n",
        "\n",
        "# This notebook is built on the t5-small checkpoint from the Hugging Face Model Hub\n",
        "MODEL_CHECKPOINT = \"t5-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yx-HCmO95AM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"xsum\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ge32TeNIGeC"
      },
      "outputs": [],
      "source": [
        "print(raw_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqEKJofd-d-N"
      },
      "outputs": [],
      "source": [
        "raw_datasets = raw_datasets.train_test_split(\n",
        "    train_size=TRAIN_TEST_SPLIT, test_size=TRAIN_TEST_SPLIT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcpPwNCF-eE-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEBGAtFFANir"
      },
      "outputs": [],
      "source": [
        "if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"summarize: \"\n",
        "else:\n",
        "    prefix = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0FRafaKAQDD"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIYEMteiATMa"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMEdpEmNAV40"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbgDmEwIAYh2"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0H78_CdA0yz"
      },
      "outputs": [],
      "source": [
        "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "generation_dataset = (\n",
        "    tokenized_datasets[\"test\"]\n",
        "    .shuffle()\n",
        "    .select(list(range(200)))\n",
        "    .to_tf_dataset(\n",
        "        batch_size=BATCH_SIZE,\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXn20_PwA4GF"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eclduE5RBH1_"
      },
      "outputs": [],
      "source": [
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JhXPDS3A9My"
      },
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "\n",
        "rouge_l = keras_nlp.metrics.RougeL()\n",
        "\n",
        "\n",
        "def metric_fn(eval_predictions):\n",
        "    predictions, labels = eval_predictions\n",
        "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    for label in labels:\n",
        "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge_l(decoded_labels, decoded_predictions)\n",
        "    # We will print only the F1 score, you can use other aggregation metrics as well\n",
        "    result = {\"RougeL\": result[\"f1_score\"]}\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MGlmIZoPBAJz",
        "outputId": "4eb26803-56d7-4ecd-b82a-4e0dc906e701"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:No label_cols specified for KerasMetricCallback, assuming you want the 'labels' key.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  76/2551 [..............................] - ETA: 32:09:15 - loss: 3.5277"
          ]
        }
      ],
      "source": [
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n",
        ")\n",
        "\n",
        "callbacks = [metric_callback]\n",
        "\n",
        "# For now we will use our test set as our validation_data\n",
        "model.fit(\n",
        "    train_dataset, validation_data=test_dataset, epochs=MAX_EPOCHS, callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6uWOSL9wBAzF",
        "outputId": "13422d8a-c1a9-4daa-88d3-cf31b1365752"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1223 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': 'Wales face the Republic of Ireland in a World Cup qualifier in Dublin on Friday.'}]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
        "\n",
        "summarizer(\n",
        "    raw_datasets[\"test\"][0][\"document\"],\n",
        "    min_length=MIN_TARGET_LENGTH,\n",
        "    max_length=MAX_TARGET_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2T_WU0PxJ48G",
        "outputId": "9e50c271-b6ab-470e-dbf9-595ba442d54d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual text:\n",
            "The ban applies to all new cosmetics and their ingredients sold in the EU, regardless of where in the world testing on animals was carried out.\n",
            "The 27 EU countries have had a ban on such tests in place since 2009. But the EU Commission is now asking the EU's trading partners to do the same.\n",
            "Animal rights lobbyists said EU officials had \"listened to the people\".\n",
            "The anti-vivisection group BUAV and the European Coalition to End Animal Experiments (ECEAE) said they had spent more than 20 years campaigning on the issue and had enlisted celebrities including Sir Paul McCartney, Morrissey and Sienna Miller to their cause. They congratulated the EU Commission for putting the ban into effect.\n",
            "But BUAV says many countries in the world still test on animals for cosmetics and the group is now pressing for a global ban.\n",
            "Mice and rats are used for more than half of all lab animal tests carried out in the EU.\n",
            "Despite the EU's 2009 ban, cosmetics firms were allowed to continue testing on animals for the most complex human health effects, such as toxicity which might lead to cancer. However, those tests now come under the ban too.\n",
            "The EU Commission says it is working with industry to develop more alternatives to animal testing, and that it allocated 238m euros (Â£208m; $310m) in 2007-2011 for such research.\n",
            "Cosmetics firms are concerned that the ban could put Europe at a competitive disadvantage in a global market.\n",
            "Cosmetics Europe chief Bertil Heerink, quoted by the Associated Press news agency, said that \"by implementing the ban at this time, the European Union is jeopardising the industry's ability to innovate\".\n",
            "----------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        " print(\"Actual text:\\n\" + raw_datasets[\"train\"][5][\"document\"])\n",
        " print(\"----------------------------------------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cZwBObnkESQB",
        "outputId": "5d394bd5-35d9-4b06-d19c-d1b1c75c578c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated summary:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': 'A new hospital is to be built on a plot of land between Fabian Way and Prince of Wales dock.'}]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " print(\"Generated summary:\")\n",
        " summarizer(\n",
        "    raw_datasets[\"train\"][5][\"document\"],\n",
        "    min_length=MIN_TARGET_LENGTH,\n",
        "    max_length=MAX_TARGET_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hiCfYgDYEvzu",
        "outputId": "16faa4f1-1e4d-45b9-dd3d-c7c4a3526b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual Summary:\n",
            "\n",
            "\n",
            "A complete ban on the sale of cosmetics developed through animal testing has taken effect in the EU.\n"
          ]
        }
      ],
      "source": [
        "print(\"Actual Summary:\\n\")\n",
        "print(\"\\n\" + raw_datasets[\"train\"][5][\"summary\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ba7aM4X9GFsG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}