{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36137ac9",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/https://colab.research.google.com/github/ianuragbhatt/text-summarization/blob/main/ts_abstractive_GPT2.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563e4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets -y\n",
    "# !pip install -q tensorflow-gpu -y\n",
    "# !pip upgrade -q numpy scipy -y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81ccea",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ec0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ed6cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 15:59:54.863942: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-11 15:59:54.889923: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-11 15:59:55.320553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924ca61",
   "metadata": {},
   "source": [
    "## Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba9b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing datasets \n",
    "data_path = 'assets/datasets/sample_findsum/'\n",
    "train_data_path = data_path + \"sample_findsum_train.csv\"\n",
    "test_data_path = data_path + \"sample_findsum_test.csv\"\n",
    "val_data_path = data_path + \"sample_findsum_val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d091ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "val_data = pd.read_csv(val_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918e850a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1322, 2), (414, 2), (331, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec7a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = load_dataset(\"csv\", data_files=data_path + \"sample_findsum_train.csv\")\n",
    "# test_dataset = load_dataset(\"csv\", data_files=data_path + \"sample_findsum_test.csv\")\n",
    "# val_dataset = load_dataset(\"csv\", data_files=data_path + \"sample_findsum_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c40198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef011b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd9ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", max_split_size_mb=20)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a0a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abda75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc605b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encodings = tokenizer(list(train_data[\"document\"]), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "y_train_encodings = tokenizer(list(train_data[\"summary\"]), padding=True, truncation=True, max_length=64, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44cffabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(X_train_encodings, y_train_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22f3006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_encodings = tokenizer(list(val_data[\"document\"]), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "y_val_encodings = tokenizer(list(val_data[\"summary\"]), padding=True, truncation=True, max_length=64, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "625b6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = Dataset(X_val_encodings, y_val_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8356fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encodings = tokenizer(list(test_data[\"document\"]), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "y_test_encodings = tokenizer(list(test_data[\"summary\"]), padding=True, truncation=True, max_length=64, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76cad9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset(X_test_encodings, y_test_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d914265",
   "metadata": {},
   "source": [
    "## Finetuned the GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01f760e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "\n",
    "# from nltk.translate.bleu_score import corpus_bleu\n",
    "# from transformers import EvalPrediction\n",
    "\n",
    "# define custom evaluation function\n",
    "# def corpus_bleu(p: EvalPrediction):\n",
    "#     references = [refs for refs in p.label_ids]\n",
    "#     hypotheses = [hyps for hyps in p.predictions]\n",
    "#     bleu_scores = corpus_bleu(references, hypotheses)\n",
    "#     return {\"bleu\": bleu_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "922a8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"assets/results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='assets/logs',\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True # This line enables mixed precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73b028b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_set,\n",
    "    eval_dataset=val_set,\n",
    "    data_collator=data_collator,\n",
    "#     compute_metrics=corpus_bleu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb03952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u1528082/miniconda3/envs/tf2/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a1255c649240a088c9cf2cea4f1a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21719/2779342431.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_21719/2779342431.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.481, 'learning_rate': 4.96e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ffb71a953a4d15996e1b86030c250d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0985708236694336, 'eval_runtime': 1.5787, 'eval_samples_per_second': 209.667, 'eval_steps_per_second': 105.15, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21719/2779342431.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_21719/2779342431.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0273, 'learning_rate': 1.982968369829684e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bd8dc120c84212b663fc7f6a04dceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.966459035873413, 'eval_runtime': 1.7768, 'eval_samples_per_second': 186.295, 'eval_steps_per_second': 93.429, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21719/2779342431.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_21719/2779342431.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 86.2976, 'train_samples_per_second': 30.638, 'train_steps_per_second': 15.319, 'train_loss': 3.1607398236573374, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1322, training_loss=3.1607398236573374, metrics={'train_runtime': 86.2976, 'train_samples_per_second': 30.638, 'train_steps_per_second': 15.319, 'train_loss': 3.1607398236573374, 'epoch': 2.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# free up GPU memory\n",
    "# del train_data, val_data, trainer, model, tokenizer\n",
    "# del train_data, val_data, model, tokenizer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f22a53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21719/2779342431.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_21719/2779342431.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565d5a882d224271bad43d94dfbabb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.908712148666382,\n",
       " 'eval_runtime': 2.1165,\n",
       " 'eval_samples_per_second': 195.605,\n",
       " 'eval_steps_per_second': 97.802,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a6bd775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "tokenizer.save_pretrained(\"assets/finetuned_gpt2_model\")\n",
    "model.save_pretrained(\"assets/finetuned_gpt2_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15df58",
   "metadata": {},
   "source": [
    "## Check BLEU Score of test dataset\n",
    "* We will use BLEU Score and see how good our model is generating summarization.\n",
    "* I will use `corpus_bleu` from `nltk` so, that I will get one `bleu_score` of whole test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88a78b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edb89240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "data_path = 'assets/datasets/sample_findsum/'\n",
    "test_data_path = data_path + \"sample_findsum_test.csv\"\n",
    "test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c43d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to saved model\n",
    "# model_path = \"./finetuned_gpt2_model\"\n",
    "model_path = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "998f21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "# set the padding token to the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0db2c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_summary(model, tokenizer, text):\n",
    "    # encode the input sequence to a maximum length of 512 tokens\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, max_length=128, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "    # generate the summary using the model\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=129,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # decode the summary tokens\n",
    "    summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "    \n",
    "    # decode the input tokens\n",
    "    decoded_inputs = tokenizer.decode(inputs['input_ids'][0])\n",
    "    return decoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad42aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the reference and predicted summaries\n",
    "ref_summaries = [[ref] for ref in test_df['summary'].tolist()]\n",
    "pred_summaries = [predict_summary(model, tokenizer, text) for text in test_df['document'].tolist()]\n",
    "pred_summaries = [[pred] for pred in pred_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdddf6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0\n"
     ]
    }
   ],
   "source": [
    "# calculate the BLEU score\n",
    "bleu_score = corpus_bleu(ref_summaries, pred_summaries)\n",
    "print(f\"BLEU score: {bleu_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56280d8d",
   "metadata": {},
   "source": [
    "## Generating paragraph summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "971a99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def compute_bleu_score(preds, targets):\n",
    "    bleu_scores = []\n",
    "    smoothing_func = SmoothingFunction().method1\n",
    "    for pred, target in zip(preds, targets):\n",
    "        pred = pred.replace('<s>', '').replace('</s>', '').strip()\n",
    "        target = target.replace('<s>', '').replace('</s>', '').strip()\n",
    "        pred_tokens = pred.split()\n",
    "        target_tokens = target.split()\n",
    "        bleu_score = sentence_bleu([target_tokens], pred_tokens, smoothing_function=smoothing_func)\n",
    "        bleu_scores.append(bleu_score)\n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    return {'bleu_score': avg_bleu_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4f69b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''\n",
    "\"on september 5 , 2012 , we acquired tog , a precision machined metal and alloy parts provider to original \n",
    "equipment manufacturers for the steam and natural gas turbine power generation market.\n",
    "the addition of koontz-wagner 's engineered packaged control house solutions expanded our \n",
    "products portfolio to our current customers , and supports the global expansion into \n",
    "adjacent markets such as oil and gas pipelines . the acquisition of tog expanded our \n",
    "products portfolio to serve the steam turbine market and , combined with our consolidated \n",
    "fabricators business unit , established a growth platform for aftermarket energy parts sales .\n",
    "the tog repair and replacement parts business provides a relatively stable revenue stream .\n",
    "the financial results of the koontz-wagner acquisition and the tog acquisition have been included\n",
    "in our product solutions segment .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e17975a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03339aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 177, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(input_ids, max_length=100, do_sample=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0450acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c9a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:  0.008458173528027359 \n",
      "\n",
      "Paragraph: \n",
      " \n",
      "\"on september 5 , 2012 , we acquired tog , a precision machined metal and alloy parts provider to original \n",
      "equipment manufacturers for the steam and natural gas turbine power generation market.\n",
      "the addition of koontz-wagner 's engineered packaged control house solutions expanded our \n",
      "products portfolio to our current customers , and supports the global expansion into \n",
      "adjacent markets such as oil and gas pipelines . the acquisition of tog expanded our \n",
      "products portfolio to serve the steam turbine market and , combined with our consolidated \n",
      "fabricators business unit , established a growth platform for aftermarket energy parts sales .\n",
      "the tog repair and replacement parts business provides a relatively stable revenue stream .\n",
      "the financial results of the koontz-wagner acquisition and the tog acquisition have been included\n",
      "in our product solutions segment .\n",
      "\n",
      "Summary: \n",
      " \n",
      "\"on september 5, 2012, we acquired tog, a precision machined metal and alloy parts provider to original \n",
      "equipment manufacturers for the steam and natural gas turbine power generation market.\n",
      "the addition of koontz-wagner's engineered packaged control house solutions expanded our \n",
      "products portfolio to our current customers, and supports the global expansion into \n",
      "adjacent markets such as oil and gas pipelines. the acquisition of tog expanded our \n",
      "products portfolio to serve the steam turbine market and, combined with our consolidated \n",
      "fabricators business unit, established a growth platform for aftermarket energy parts sales.\n",
      "the tog repair and replacement parts business provides a relatively stable revenue stream.\n",
      "the financial results of the koontz-wagner acquisition and the tog acquisition have been included\n",
      "in our product solutions segment.\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(\"BLEU Score: \", compute_bleu_score(input_text, summary)[\"bleu_score\"], \"\\n\")\n",
    "print(\"Paragraph: \\n\", input_text)\n",
    "print(\"Summary: \\n\", summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "543ce938",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* GPT2 finedtuned and without finetuning is not working well on `FindSum` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb2856",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
