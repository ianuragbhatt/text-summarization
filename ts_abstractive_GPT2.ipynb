{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36137ac9",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/https://colab.research.google.com/github/ianuragbhatt/text-summarization/blob/main/ts_abstractive_GPT2.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets -y\n",
    "# !pip install -q tensorflow-gpu -y\n",
    "# !pip upgrade -q numpy scipy -y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81ccea",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924ca61",
   "metadata": {},
   "source": [
    "## Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing datasets \n",
    "data_path = 'assets/datasets/sample_findsum/'\n",
    "train_data_path = data_path + \"sample_findsum_train.csv\"\n",
    "test_data_path = data_path + \"sample_findsum_test.csv\"\n",
    "val_data_path = data_path + \"sample_findsum_val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d091ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "val_data = pd.read_csv(val_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = load_dataset(\"csv\", data_files=data_path + \"sample_findsum_train.csv\")\n",
    "# test_dataset = load_dataset(\"csv\", data_files=data_path + \"sample_findsum_test.csv\")\n",
    "# val_dataset = load_dataset(\"csv\", data_files=data_path + \"sample_findsum_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c40198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef011b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", max_split_size_mb=20)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc605b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encodings = tokenizer(list(train_data[\"document\"]), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "y_train_encodings = tokenizer(list(train_data[\"summary\"]), padding=True, truncation=True, max_length=64, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cffabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(X_train_encodings, y_train_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_encodings = tokenizer(list(val_data[\"document\"]), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "y_val_encodings = tokenizer(list(val_data[\"summary\"]), padding=True, truncation=True, max_length=64, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = Dataset(X_val_encodings, y_val_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encodings = tokenizer(list(test_data[\"document\"]), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "y_test_encodings = tokenizer(list(test_data[\"summary\"]), padding=True, truncation=True, max_length=64, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cad9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset(X_test_encodings, y_test_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d914265",
   "metadata": {},
   "source": [
    "## Finetuned the GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f760e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "\n",
    "# from nltk.translate.bleu_score import corpus_bleu\n",
    "# from transformers import EvalPrediction\n",
    "\n",
    "# define custom evaluation function\n",
    "# def corpus_bleu(p: EvalPrediction):\n",
    "#     references = [refs for refs in p.label_ids]\n",
    "#     hypotheses = [hyps for hyps in p.predictions]\n",
    "#     bleu_scores = corpus_bleu(references, hypotheses)\n",
    "#     return {\"bleu\": bleu_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"assets/results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='assets/logs',\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True, # This line enables mixed precision training\n",
    "    regularization_params={'l2': 0.01},\n",
    "    callbacks=[EarlyStoppingCallback(patience=4, min_delta=0.01, mode='min')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b028b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_set,\n",
    "    eval_dataset=val_set,\n",
    "    data_collator=data_collator\n",
    "#     compute_metrics=corpus_bleu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# free up GPU memory\n",
    "# del train_data, val_data, trainer, model, tokenizer\n",
    "# del train_data, val_data, model, tokenizer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a53e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bd775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "tokenizer.save_pretrained(\"assets/finetuned_gpt2_model\")\n",
    "model.save_pretrained(\"assets/finetuned_gpt2_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15df58",
   "metadata": {},
   "source": [
    "## Check BLEU Score of test dataset\n",
    "* We will use BLEU Score and see how good our model is generating summarization.\n",
    "* I will use `corpus_bleu` from `nltk` so, that I will get one `bleu_score` of whole test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a78b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb89240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "data_path = 'assets/datasets/sample_findsum/'\n",
    "test_data_path = data_path + \"sample_findsum_test.csv\"\n",
    "test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to saved model\n",
    "# model_path = \"./finetuned_gpt2_model\"\n",
    "model_path = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "# set the padding token to the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_summary(model, tokenizer, text):\n",
    "    # encode the input sequence to a maximum length of 512 tokens\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, max_length=128, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "    # generate the summary using the model\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=129,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # decode the summary tokens\n",
    "    summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "    \n",
    "    # decode the input tokens\n",
    "    decoded_inputs = tokenizer.decode(inputs['input_ids'][0])\n",
    "    return decoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad42aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# tokenize the reference and predicted summaries\n",
    "ref_summaries = [[ref] for ref in test_df['summary'].tolist()]\n",
    "pred_summaries = [predict_summary(model, tokenizer, text) for text in tqdm(test_df['document'].tolist())]\n",
    "pred_summaries = [[pred] for pred in pred_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdddf6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the BLEU score\n",
    "bleu_score = corpus_bleu(ref_summaries, pred_summaries)\n",
    "print(f\"BLEU score: {bleu_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56280d8d",
   "metadata": {},
   "source": [
    "## Generating paragraph summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def compute_bleu_score(preds, targets):\n",
    "    bleu_scores = []\n",
    "    smoothing_func = SmoothingFunction().method1\n",
    "    for pred, target in zip(preds, targets):\n",
    "        pred = pred.replace('<s>', '').replace('</s>', '').strip()\n",
    "        target = target.replace('<s>', '').replace('</s>', '').strip()\n",
    "        pred_tokens = pred.split()\n",
    "        target_tokens = target.split()\n",
    "        bleu_score = sentence_bleu([target_tokens], pred_tokens, smoothing_function=smoothing_func)\n",
    "        bleu_scores.append(bleu_score)\n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    return {'bleu_score': avg_bleu_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f69b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''\n",
    "\"on september 5 , 2012 , we acquired tog , a precision machined metal and alloy parts provider to original \n",
    "equipment manufacturers for the steam and natural gas turbine power generation market.\n",
    "the addition of koontz-wagner 's engineered packaged control house solutions expanded our \n",
    "products portfolio to our current customers , and supports the global expansion into \n",
    "adjacent markets such as oil and gas pipelines . the acquisition of tog expanded our \n",
    "products portfolio to serve the steam turbine market and , combined with our consolidated \n",
    "fabricators business unit , established a growth platform for aftermarket energy parts sales .\n",
    "the tog repair and replacement parts business provides a relatively stable revenue stream .\n",
    "the financial results of the koontz-wagner acquisition and the tog acquisition have been included\n",
    "in our product solutions segment .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17975a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03339aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(input_ids, max_length=100, do_sample=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BLEU Score: \", compute_bleu_score(input_text, summary)[\"bleu_score\"], \"\\n\")\n",
    "print(\"Paragraph: \\n\", input_text)\n",
    "print(\"Summary: \\n\", summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "543ce938",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* GPT2 finedtuned and without finetuning is not working well on `FindSum` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb2856",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
